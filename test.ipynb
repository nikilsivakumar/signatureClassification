{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Signature Verification Model using Vgg16 and CNN-Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for files selection\n",
    "import glob\n",
    "\n",
    "gen = [glob.glob('data/Dataset_Signature_Final/Dataset/dataset1/real/*.*'),\n",
    "       glob.glob('data/Dataset_Signature_Final/Dataset/dataset2/real/*.*'),\n",
    "       glob.glob('data/Dataset_Signature_Final/Dataset/dataset3/real/*.*'),\n",
    "       glob.glob('data/Dataset_Signature_Final/Dataset/dataset4/real1/*.*')]\n",
    "                 \n",
    "forg = [glob.glob('data/Dataset_Signature_Final/Dataset/dataset1/forge/*.*'),\n",
    "        glob.glob('data/Dataset_Signature_Final/Dataset/dataset2/forge/*.*'),\n",
    "        glob.glob('data/Dataset_Signature_Final/Dataset/dataset3/forge/*.*'),\n",
    "        glob.glob('data/Dataset_Signature_Final/Dataset/dataset4/forge/*.*')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data = []\n",
    "train_labels = []\n",
    "test_data = []\n",
    "test_labels = []\n",
    "\n",
    "#Gen and forg are lists containing image paths\n",
    "all_data = []  # Combine all images\n",
    "all_labels = []  # Combine all labels\n",
    "\n",
    "for data in gen:\n",
    "    for i in data:\n",
    "        image = cv2.imread(i)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = cv2.resize(image, (224, 224))\n",
    "        all_data.append(image)\n",
    "        all_labels.append(0)  # Genuine = 0\n",
    "\n",
    "for data in forg:\n",
    "    for j in data:\n",
    "        image = cv2.imread(j)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = cv2.resize(image, (224, 224))\n",
    "        all_data.append(image)\n",
    "        all_labels.append(1)  # Forged = 1\n",
    "\n",
    "# Split data into training and testing sets \n",
    "X_train, X_test, y_train, y_test = train_test_split(all_data, all_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "train_data = np.array(X_train) / 255.0\n",
    "train_labels = np.array(y_train)\n",
    "test_data = np.array(X_test) / 255.0\n",
    "test_labels = np.array(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(576, 224, 224, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(144, 224, 224, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(144,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.16.1'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vgg16 Pre Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m297s\u001b[0m 10s/step - accuracy: 0.4591 - loss: 0.7845 - precision: 0.4289 - recall: 0.4497 - val_accuracy: 0.4914 - val_loss: 0.7309 - val_precision: 0.4914 - val_recall: 1.0000\n",
      "Epoch 2/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m289s\u001b[0m 10s/step - accuracy: 0.5157 - loss: 0.7086 - precision: 0.5248 - recall: 0.7048 - val_accuracy: 0.4914 - val_loss: 0.7059 - val_precision: 0.4914 - val_recall: 1.0000\n",
      "Epoch 3/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m293s\u001b[0m 10s/step - accuracy: 0.5820 - loss: 0.6913 - precision: 0.6033 - recall: 0.8060 - val_accuracy: 0.4914 - val_loss: 0.6960 - val_precision: 0.4914 - val_recall: 1.0000\n",
      "Epoch 4/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m288s\u001b[0m 10s/step - accuracy: 0.4801 - loss: 0.6933 - precision: 0.4901 - recall: 0.6303 - val_accuracy: 0.4914 - val_loss: 0.6935 - val_precision: 0.4914 - val_recall: 1.0000\n",
      "Epoch 5/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m292s\u001b[0m 10s/step - accuracy: 0.5523 - loss: 0.6912 - precision: 0.5611 - recall: 0.9569 - val_accuracy: 0.4914 - val_loss: 0.6937 - val_precision: 0.4914 - val_recall: 1.0000\n",
      "Epoch 6/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m286s\u001b[0m 10s/step - accuracy: 0.4490 - loss: 0.6946 - precision: 0.4668 - recall: 0.6577 - val_accuracy: 0.4914 - val_loss: 0.6933 - val_precision: 0.4914 - val_recall: 1.0000\n",
      "Epoch 7/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m290s\u001b[0m 10s/step - accuracy: 0.5397 - loss: 0.6930 - precision: 0.5288 - recall: 0.9735 - val_accuracy: 0.4914 - val_loss: 0.6932 - val_precision: 0.4914 - val_recall: 1.0000\n",
      "Epoch 8/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m296s\u001b[0m 10s/step - accuracy: 0.5306 - loss: 0.6928 - precision: 0.5306 - recall: 1.0000 - val_accuracy: 0.4914 - val_loss: 0.6934 - val_precision: 0.4914 - val_recall: 1.0000\n",
      "Epoch 9/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m309s\u001b[0m 11s/step - accuracy: 0.5308 - loss: 0.6926 - precision: 0.5308 - recall: 1.0000 - val_accuracy: 0.4914 - val_loss: 0.6934 - val_precision: 0.4914 - val_recall: 1.0000\n",
      "Epoch 10/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m317s\u001b[0m 11s/step - accuracy: 0.5171 - loss: 0.6927 - precision: 0.5171 - recall: 1.0000 - val_accuracy: 0.4914 - val_loss: 0.6935 - val_precision: 0.4914 - val_recall: 1.0000\n",
      "Epoch 10: early stopping\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3s/step\n",
      "Predictions: [[1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 3s/step - accuracy: 0.4278 - loss: 0.6952 - precision: 0.4278 - recall: 1.0000\n",
      "Test Accuracy: 0.4305555522441864\n",
      "Test Precision: 0.4305555522441864\n",
      "Test Recall: 1.0\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "\n",
    "# Define image dimensions\n",
    "img_width, img_height = 224, 224\n",
    "\n",
    "# Data normalization\n",
    "train_data = train_data.astype('float32') / 255.0\n",
    "test_data = test_data.astype('float32') / 255.0\n",
    "\n",
    "# Preprocessing for VGG16 \n",
    "train_datagen = ImageDataGenerator(rescale=1./255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow(train_data,train_labels,  \n",
    "        # target_size=(img_width, img_height),\n",
    "        batch_size=32\n",
    "        # class_mode='binary'  \n",
    ")\n",
    "\n",
    "\n",
    "test_generator = test_datagen.flow(test_data,test_labels, \n",
    "        # target_size=(img_width, img_height),\n",
    "        batch_size=32  \n",
    "        # class_mode='binary'\n",
    ")\n",
    "\n",
    "\n",
    "# Load the VGG16 model (exclude the top layers)\n",
    "vgg16_model = VGG16(weights='imagenet', include_top=False, input_shape=(img_width, img_height, 3))\n",
    "\n",
    "# Freeze pre-trained VGG16 layers, here we made it True for better results\n",
    "for layer in vgg16_model.layers:\n",
    "    layer.trainable = True\n",
    "\n",
    "# Add custom layers for classification\n",
    "x = vgg16_model.output\n",
    "x = Flatten()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(1, activation='sigmoid')(x)  \n",
    "\n",
    "# Create the final model\n",
    "model = Model(inputs=vgg16_model.input, outputs=x)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.0001), metrics=['accuracy','precision', 'recall'])\n",
    "\n",
    "# Early stopping \n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, verbose=1)\n",
    "\n",
    "# Adjust batch size and epochs as needed\n",
    "model.fit(train_data, train_labels, epochs=10, batch_size=16, validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "\n",
    "# Make predictions on test data\n",
    "predictions = model.predict(test_data)\n",
    "\n",
    "# Threshold predictions \n",
    "predictions = (predictions > 0.5).astype(int)  \n",
    "\n",
    "print('Predictions:', predictions)\n",
    "\n",
    "test_loss, test_acc, test_precision, test_recall = model.evaluate(test_data, test_labels)\n",
    "\n",
    "print('Test Accuracy:', test_acc)\n",
    "print('Test Precision:', test_precision)\n",
    "print('Test Recall:', test_recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vgg16 using SGD Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m282s\u001b[0m 10s/step - accuracy: 0.4476 - loss: 0.9101 - precision: 0.4409 - recall: 0.4637 - val_accuracy: 0.4914 - val_loss: 0.7061 - val_precision: 0.4914 - val_recall: 1.0000\n",
      "Epoch 2/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m276s\u001b[0m 10s/step - accuracy: 0.5189 - loss: 0.6978 - precision: 0.5326 - recall: 0.8369 - val_accuracy: 0.4914 - val_loss: 0.6942 - val_precision: 0.4914 - val_recall: 1.0000\n",
      "Epoch 3/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m288s\u001b[0m 10s/step - accuracy: 0.4995 - loss: 0.6940 - precision: 0.4963 - recall: 0.6695 - val_accuracy: 0.4914 - val_loss: 0.7086 - val_precision: 0.4914 - val_recall: 1.0000\n",
      "Epoch 4/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m313s\u001b[0m 11s/step - accuracy: 0.5589 - loss: 0.6912 - precision: 0.5550 - recall: 0.8389 - val_accuracy: 0.4914 - val_loss: 0.6938 - val_precision: 0.4914 - val_recall: 1.0000\n",
      "Epoch 5/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m275s\u001b[0m 9s/step - accuracy: 0.5519 - loss: 0.6900 - precision: 0.5461 - recall: 0.8480 - val_accuracy: 0.4914 - val_loss: 0.6934 - val_precision: 0.4914 - val_recall: 1.0000\n",
      "Epoch 6/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m269s\u001b[0m 9s/step - accuracy: 0.5211 - loss: 0.6934 - precision: 0.5388 - recall: 0.7312 - val_accuracy: 0.4914 - val_loss: 0.6932 - val_precision: 0.4914 - val_recall: 1.0000\n",
      "Epoch 7/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m268s\u001b[0m 9s/step - accuracy: 0.5000 - loss: 0.6932 - precision: 0.5040 - recall: 0.9670 - val_accuracy: 0.4914 - val_loss: 0.6934 - val_precision: 0.4914 - val_recall: 1.0000\n",
      "Epoch 8/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m269s\u001b[0m 9s/step - accuracy: 0.5235 - loss: 0.6929 - precision: 0.5235 - recall: 1.0000 - val_accuracy: 0.4914 - val_loss: 0.6936 - val_precision: 0.4914 - val_recall: 1.0000\n",
      "Epoch 9/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m271s\u001b[0m 9s/step - accuracy: 0.5268 - loss: 0.6925 - precision: 0.5268 - recall: 1.0000 - val_accuracy: 0.4914 - val_loss: 0.6939 - val_precision: 0.4914 - val_recall: 1.0000\n",
      "Epoch 9: early stopping\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 3s/step\n",
      "Predictions: [[1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 3s/step - accuracy: 0.4278 - loss: 0.6970 - precision: 0.4278 - recall: 1.0000\n",
      "Test Accuracy: 0.4305555522441864\n",
      "Test Precision: 0.4305555522441864\n",
      "Test Recall: 1.0\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from keras import layers\n",
    "from keras import regularizers\n",
    "\n",
    "\n",
    "# Define image dimensions\n",
    "img_width, img_height = 224, 224\n",
    "\n",
    "# Data normalization \n",
    "train_data = train_data.astype('float32') / 255.0\n",
    "test_data = test_data.astype('float32') / 255.0\n",
    "\n",
    "# Preprocessing for VGG16, few more pre processors added  \n",
    "train_datagen = ImageDataGenerator(rescale=1./255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True,\n",
    "                                    channel_shift_range=50,rotation_range=20,width_shift_range=0.2,\n",
    "                                    height_shift_range=0.2)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow(train_data,train_labels,  \n",
    "        # target_size=(img_width, img_height),\n",
    "        batch_size=32  \n",
    "        # class_mode='binary' \n",
    ")\n",
    "\n",
    "\n",
    "test_generator = test_datagen.flow(test_data,test_labels,  \n",
    "        # target_size=(img_width, img_height),\n",
    "        batch_size=32 \n",
    "        # class_mode='binary' \n",
    "\n",
    "\n",
    "# Load the VGG16 model \n",
    "vgg16_model = VGG16(weights='imagenet', include_top=False, input_shape=(img_width, img_height, 3))\n",
    "\n",
    "# Freeze pre-trained VGG16 layers \n",
    "for layer in vgg16_model.layers:\n",
    "    layer.trainable = True\n",
    "\n",
    "# Add custom layers for classification\n",
    "x = vgg16_model.output\n",
    "x = Flatten()(x)\n",
    "#x = Dense(1024, activation='relu')(x)\n",
    "x = Dense(1024, activation='relu')(x) \n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(1, activation='sigmoid')(x) \n",
    "\n",
    "# Create the final model\n",
    "model = Model(inputs=vgg16_model.input, outputs=x)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer=SGD(learning_rate=0.001,momentum=0.9), metrics=['accuracy','precision', 'recall'])\n",
    "\n",
    "# Early stopping (optional)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, verbose=1)\n",
    "\n",
    "# Train with your data (adjust batch size and epochs as needed)\n",
    "model.fit(train_data, train_labels, epochs=10, batch_size=16, validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "\n",
    "# Make predictions on test data\n",
    "predictions = model.predict(test_data)\n",
    "\n",
    "# Threshold predictions \n",
    "predictions = (predictions > 0.5).astype(int) \n",
    "\n",
    "print('Predictions:', predictions)\n",
    "\n",
    "test_loss, test_acc, test_precision, test_recall = model.evaluate(test_data, test_labels)\n",
    "\n",
    "print('Test Accuracy:', test_acc)\n",
    "print('Test Precision:', test_precision)\n",
    "print('Test Recall:', test_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Keras-Preprocessing\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: numpy>=1.9.1 in c:\\users\\nikil\\anaconda3\\envs\\pytorch\\lib\\site-packages (from Keras-Preprocessing) (1.26.3)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\nikil\\anaconda3\\envs\\pytorch\\lib\\site-packages (from Keras-Preprocessing) (1.16.0)\n",
      "Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Installing collected packages: Keras-Preprocessing\n",
      "Successfully installed Keras-Preprocessing-1.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install Keras-Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN - Relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nikil\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">222</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">222</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,792</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">74</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">74</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">72</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">72</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,464</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">36</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">36</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">41472</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">5,308,544</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">258</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_4 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m222\u001b[0m, \u001b[38;5;34m222\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │         \u001b[38;5;34m1,792\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_4 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m74\u001b[0m, \u001b[38;5;34m74\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_5 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m72\u001b[0m, \u001b[38;5;34m72\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │        \u001b[38;5;34m18,464\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_5 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m36\u001b[0m, \u001b[38;5;34m36\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_5 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m41472\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_9 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │     \u001b[38;5;34m5,308,544\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_10 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │           \u001b[38;5;34m258\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,329,058</span> (20.33 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,329,058\u001b[0m (20.33 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,329,058</span> (20.33 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,329,058\u001b[0m (20.33 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m27/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m19s\u001b[0m 10s/step - accuracy: 0.5322 - loss: 0.6910 - precision: 0.5322 - recall: 1.0000"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D,MaxPooling2D,Flatten,Dense,Dropout\n",
    "from keras_preprocessing.image import ImageDataGenerator\n",
    "from sklearn.metrics import confusion_matrix as CM\n",
    "from keras.optimizers import Adam\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau, TensorBoard\n",
    "\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=1, keepdims=True)\n",
    "\n",
    "def objective(x, y):\n",
    " return x**2.0 + y**2.0\n",
    "\n",
    "\n",
    "network = Sequential()\n",
    "\n",
    "network.add(Conv2D(64,(3,3),input_shape=(224,224,3),activation='relu'))\n",
    "network.add(MaxPooling2D(3,3))\n",
    "\n",
    "network.add(Conv2D(32,(3,3),activation='relu'))\n",
    "network.add(MaxPooling2D(2,2))\n",
    "\n",
    "network.add(Flatten()) #NX1\n",
    "network.add(Dense(128,activation = 'relu'))\n",
    "network.add(Dropout(rate=0.3)) #143\n",
    "\n",
    "network.add(Dense(2,activation = 'softmax'))\n",
    "\n",
    "network.compile(optimizer=Adam(learning_rate=0.001),loss=\"binary_crossentropy\",metrics=['accuracy','precision', 'recall'])\n",
    "network.summary()\n",
    "\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, verbose=1)\n",
    "callback_early_stop_reduceLROnPlateau=[early_stopping]\n",
    "model.fit(train_data, train_labels, epochs=10, batch_size=16, validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "predictions = model.predict(test_data)\n",
    "predictions = (predictions > 0.5).astype(int) \n",
    "\n",
    "print('Predictions:', predictions)\n",
    "test_loss, test_acc, test_precision, test_recall = model.evaluate(test_data, test_labels)\n",
    "print('Test Accuracy:', test_acc)\n",
    "print('Test Precision:', test_precision)\n",
    "print('Test Recall:', test_recall)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m314s\u001b[0m 39s/step - accuracy: 0.5268 - loss: 0.6917 - precision: 0.5268 - recall: 1.0000 - val_accuracy: 0.4914 - val_loss: 0.6955 - val_precision: 0.4914 - val_recall: 1.0000\n",
      "Epoch 2/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m314s\u001b[0m 39s/step - accuracy: 0.5310 - loss: 0.6914 - precision: 0.5310 - recall: 1.0000 - val_accuracy: 0.4914 - val_loss: 0.6954 - val_precision: 0.4914 - val_recall: 1.0000\n",
      "Epoch 3/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m320s\u001b[0m 40s/step - accuracy: 0.4973 - loss: 0.6948 - precision: 0.4973 - recall: 1.0000 - val_accuracy: 0.4914 - val_loss: 0.6954 - val_precision: 0.4914 - val_recall: 1.0000\n",
      "Epoch 4/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m321s\u001b[0m 40s/step - accuracy: 0.4975 - loss: 0.6946 - precision: 0.4975 - recall: 1.0000 - val_accuracy: 0.4914 - val_loss: 0.6953 - val_precision: 0.4914 - val_recall: 1.0000\n",
      "Epoch 5/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m323s\u001b[0m 40s/step - accuracy: 0.5199 - loss: 0.6925 - precision: 0.5199 - recall: 1.0000 - val_accuracy: 0.4914 - val_loss: 0.6953 - val_precision: 0.4914 - val_recall: 1.0000\n",
      "Epoch 6/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m322s\u001b[0m 40s/step - accuracy: 0.5187 - loss: 0.6925 - precision: 0.5187 - recall: 1.0000 - val_accuracy: 0.4914 - val_loss: 0.6953 - val_precision: 0.4914 - val_recall: 1.0000\n",
      "Epoch 7/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m317s\u001b[0m 39s/step - accuracy: 0.5187 - loss: 0.6926 - precision: 0.5187 - recall: 1.0000 - val_accuracy: 0.4914 - val_loss: 0.6953 - val_precision: 0.4914 - val_recall: 1.0000\n",
      "Epoch 8/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m308s\u001b[0m 38s/step - accuracy: 0.5236 - loss: 0.6922 - precision: 0.5236 - recall: 1.0000 - val_accuracy: 0.4914 - val_loss: 0.6953 - val_precision: 0.4914 - val_recall: 1.0000\n",
      "Epoch 8: early stopping\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau, TensorBoard\n",
    "earlyStopping = EarlyStopping(monitor='val_loss',\n",
    "                              min_delta=0,\n",
    "                              patience=3,\n",
    "                              verbose=1)\n",
    "\n",
    "callback_early_stop_reduceLROnPlateau=[earlyStopping]\n",
    "\n",
    "\n",
    "EPOCHS = 10\n",
    "BS = 64\n",
    "progess = model.fit(train_data,train_labels, batch_size=BS,epochs=EPOCHS, callbacks=callback_early_stop_reduceLROnPlateau,validation_split=.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = progess.history['accuracy']\n",
    "val_acc = progess.history['val_accuracy']\n",
    "loss = progess.history['loss']\n",
    "val_loss = progess.history['val_loss']\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
